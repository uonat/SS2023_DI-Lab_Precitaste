{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/uonat/SS2023_DI-Lab_Precitaste.git &> /dev/null\n",
    "%cd SS2023_DI-Lab_Precitaste\n",
    "%pip install . &> /dev/null\n",
    "\n",
    "import distutils.core\n",
    "import sys,os\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'  &> /dev/null\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])} &> /dev/null\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
    "\n",
    "%pip install ftfy regex tqdm &> /dev/null\n",
    "%pip install git+https://github.com/openai/CLIP.git &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.CLIP import load_model as load_clip,tokenize_text,Calculate_Scores,get_total_num_obj\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = load_clip(\"ViT-B/32\",device)\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '/content/retail_product_checkout'\n",
    "!unzip -q -j \"/content/drive/MyDrive/ApplicationProject/Data/retail-product-checkout-dataset.zip\" \"val2019/*\" -d '/content/retail_product_checkout/val2019' \n",
    "!unzip -q -j \"/content/drive/MyDrive/ApplicationProject/Data/retail-product-checkout-dataset.zip\" \"instances_val2019.json\" -d '/content/retail_product_checkout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.RPCDataset import RPCDataset\n",
    "val_dataset_path = \"/content/retail_product_checkout\"\n",
    "val_dataset = RPCDataset(val_dataset_path, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = set()\n",
    "\n",
    "for i in range(val_dataset.get_num_imgs()):\n",
    "  annots  = val_dataset.get_annots_by_img_id(i, key_for_category='sku_name')\n",
    "  for annot in annots:\n",
    "    all_labels.add(' '.join(annot[1].split('_')[1:]))\n",
    "all_labels = list(all_labels)\n",
    "all_labels_a_an = []\n",
    "for i in range(len(all_labels)):\n",
    "  if all_labels[i][0] in ('a', 'e', 'i', 'o', 'u'):\n",
    "    all_labels_a_an.append( \"an \" + all_labels[i])\n",
    "  else:\n",
    "    all_labels_a_an.append( \"a \" + all_labels[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_p = tokenize_text([\"a photo of {}\".format(s) for s in all_labels_a_an],device)\n",
    "with torch.no_grad():\n",
    "  text_features = clip_model.encode_text(text_p)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "Results_path = \"/content/drive/MyDrive/ApplicationProject/Results\"\n",
    "output_name = \"clip_gt_txt_Result_v1.pkl\"\n",
    "if os.path.isfile(os.path.join(Results_path,output_name)):\n",
    "  from datetime import datetime\n",
    "  tmp_str = str(datetime.now()).split(' ')\n",
    "  output_name = output_name.split('.')[0] + '_' + tmp_str[0] + '_' + tmp_str[1].split('.')[0].replace(\":\", \"-\") +\".pkl\"\n",
    "output_dir = os.path.join(Results_path,output_name)\n",
    "\n",
    "Results = Calculate_Scores(clip_model,preprocess,val_dataset,text_features,all_labels,output_dir,device,False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "gt_label = []\n",
    "for res in Results:\n",
    "  tmp_arr = [0] * len(all_labels)\n",
    "  tmp_arr[res[1]] = 1\n",
    "  scores += res[0]\n",
    "  gt_label += tmp_arr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images: \",len(val_dataset.get_num_imgs())) \n",
    "print(\"Number of objects: \",get_total_num_obj(val_dataset)) \n",
    "print(\"Number of classes: \",len(all_labels))\n",
    "assert len(scores) == len(gt_label)\n",
    "assert len(scores) == get_total_num_obj(val_dataset)*len(all_labels)\n",
    "print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score,average_precision_score,accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Prediction Results:\")\n",
    "\n",
    "def to_labels(pos_probs, threshold):\n",
    " return [1 if nm > threshold else 0 for nm in pos_probs]\n",
    "\n",
    "cand_thresholds = [x / 100.0 for x in range(10, 95, 5)]\n",
    "f1_scores_for_thrs = [f1_score(gt_label, to_labels(scores, t)) for t in cand_thresholds]\n",
    "ix = np.argmax(f1_scores_for_thrs)\n",
    "Th = cand_thresholds[ix]\n",
    "\n",
    "print('Threshold=%.2f, F-Score=%.5f' % Th, f1_scores_for_thrs[ix])\n",
    "print(\"precision: %.5f\" % precision_score(gt_label, to_labels(scores, Th)))\n",
    "print(\"recall: %.5f\" % recall_score(gt_label, to_labels(scores, Th)))\n",
    "print(\"f1_score: %.5f\" % f1_score(gt_label, to_labels(scores, Th)))\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "y_true = []\n",
    "y_score = []\n",
    "\n",
    "for res in Results:\n",
    "  tmp_arr = [0] * len(all_labels)\n",
    "  tmp_arr[res[1]] = 1\n",
    "  y_true.append(tmp_arr)\n",
    "\n",
    "  tmp_arr = [0] * len(all_labels)\n",
    "  tmp_arr[np.argmax(res[0])] = 1\n",
    "  y_score.append(tmp_arr)\n",
    "\n",
    "print('average_precision=%.5f, ACC=%.5f' % (average_precision_score(y_true, y_score), accuracy_score(y_true, y_score)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
