{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7e26ea38659a46f4bcf0a431aee00aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6deab917bca4562b2c142684a8e6b08",
              "IPY_MODEL_ac4eebe8ca06457d98fbe654881435b8",
              "IPY_MODEL_b88cf25573e9479c8521446f635b6e33"
            ],
            "layout": "IPY_MODEL_a8c16cdee83b46afb1f962f7dde7a267"
          }
        },
        "e6deab917bca4562b2c142684a8e6b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c692ee65ce5446ef90e413b95ecd9b0b",
            "placeholder": "​",
            "style": "IPY_MODEL_825bdb4794454af78135a8020e9caedf",
            "value": "100%"
          }
        },
        "ac4eebe8ca06457d98fbe654881435b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdbf4718d2f74f71b292cc08ab46d40a",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d80fcc66a2914ca1b89d18cd6143b1b9",
            "value": 5
          }
        },
        "b88cf25573e9479c8521446f635b6e33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96245cf32a3a465aacd71f0e286aca05",
            "placeholder": "​",
            "style": "IPY_MODEL_c8b9daab63884f5fb4d05c732d8b0f8c",
            "value": " 5/5 [16:16&lt;00:00, 193.17s/it]"
          }
        },
        "a8c16cdee83b46afb1f962f7dde7a267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c692ee65ce5446ef90e413b95ecd9b0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "825bdb4794454af78135a8020e9caedf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdbf4718d2f74f71b292cc08ab46d40a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d80fcc66a2914ca1b89d18cd6143b1b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96245cf32a3a465aacd71f0e286aca05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8b9daab63884f5fb4d05c732d8b0f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook includes all three stages of the training free RPC product recognition that implements Vith + DINO + Bbox label assignment.\n",
        "\n",
        "Models used in the algorithm took some time for inference and predicting all\n",
        "instances in the dataset might not fit into a single runtime. Therefore the whole algorithm split into three stages and each stage generate outputs for other stages to use."
      ],
      "metadata": {
        "id": "lV4APUiAH2Ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1st Stage: ViT-H Object Proposer\n",
        "\n",
        "Inputs:  Images of the dataset\n",
        "\n",
        "Outputs: All predicted bounding box objects in the form of Prediction objects, if they are matched with a gt box it will also be included in the objects.\n",
        "\n"
      ],
      "metadata": {
        "id": "lWrxjGbfJJfa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDnKlHmvWw61",
        "outputId": "32562b96-95d3-41f6-d6f3-1fc850709d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SS2023_DI-Lab_Precitaste'...\n",
            "remote: Enumerating objects: 334, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 334 (delta 27), reused 27 (delta 15), pack-reused 282\u001b[K\n",
            "Receiving objects: 100% (334/334), 30.46 MiB | 21.54 MiB/s, done.\n",
            "Resolving deltas: 100% (122/122), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/uonat/SS2023_DI-Lab_Precitaste.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/SS2023_DI-Lab_Precitaste'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7DcCB6bW0ms",
        "outputId": "87d45829-51d3-4ed9-d1f0-212a2c3d2c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SS2023_DI-Lab_Precitaste\n",
            "* \u001b[32mdinov2                            \u001b[m efba59f Adding mAP calculation to evaluation code.\n",
            "  main                              \u001b[m 02b0215 generalized version of calculate_embeddings\n",
            "  \u001b[31mremotes/origin/CLIP-implementation\u001b[m 134fe54 Updated CLIP Notebook\n",
            "  \u001b[31mremotes/origin/HEAD               \u001b[m -> origin/main\n",
            "  \u001b[31mremotes/origin/dinov2             \u001b[m efba59f Adding mAP calculation to evaluation code.\n",
            "  \u001b[31mremotes/origin/main               \u001b[m 02b0215 generalized version of calculate_embeddings\n",
            "  \u001b[31mremotes/origin/yushan             \u001b[m db636cc Updated RegionCLIP as baseline\n",
            "Already on 'dinov2'\n",
            "Your branch is up to date with 'origin/dinov2'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install . &> /dev/null"
      ],
      "metadata": {
        "id": "FmFHUpk_W7Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import distutils.core\n",
        "import sys,os\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'  &> /dev/null\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])} &> /dev/null\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwFsRhLDW-oZ",
        "outputId": "63f0f96a-6498-4509-98da-faa48c35a3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://dl.fbaipublicfiles.com/detectron2/ViTDet/LVIS/mask_rcnn_vitdet_h/332434656/model_final_866730.pkl'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF3SScl7X1Ar",
        "outputId": "ff6fe7db-3406-49b2-c780-698c23793770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-28 18:59:59--  https://dl.fbaipublicfiles.com/detectron2/ViTDet/LVIS/mask_rcnn_vitdet_h/332434656/model_final_866730.pkl\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.8.248.127, 65.8.248.107, 65.8.248.124, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.8.248.127|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2669063758 (2.5G) [binary/octet-stream]\n",
            "Saving to: ‘model_final_866730.pkl’\n",
            "\n",
            "model_final_866730. 100%[===================>]   2.49G  25.5MB/s    in 95s     \n",
            "\n",
            "2023-06-28 19:01:34 (26.9 MB/s) - ‘model_final_866730.pkl’ saved [2669063758/2669063758]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash '/content/SS2023_DI-Lab_Precitaste/scripts/download_rpc.sh'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrKO4ufNX352",
        "outputId": "981e1945-2179-4a0b-e2fa-69bff541115a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset!\n",
            "Downloading retail-product-checkout-dataset.zip to /content/SS2023_DI-Lab_Precitaste\n",
            "100% 25.3G/25.3G [05:19<00:00, 40.2MB/s]\n",
            "100% 25.3G/25.3G [05:19<00:00, 85.3MB/s]\n",
            "Unzipping dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch, detectron2\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.config import LazyConfig,instantiate\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from dataset.RPCDataset import RPCDataset\n",
        "from notebooks.utils.dino_v2 import crop_object_with_bbox, calculate_iou, find_gt_bboxes_of_pred\n",
        "from notebooks.utils.Prediction import Prediction, dump_pred_objects, get_pred_objects_per_image, read_pred_objects_json\n",
        "from utilities.non_maximum_suppression import run_nms\n",
        "from utilities.bbox_postprocess import eliminate_boxes\n",
        "\n",
        "def load_model(model_path,config_path): #mask_rcnn_vitdet_h_100ep.py\n",
        "    cfg = LazyConfig.load(config_path)\n",
        "    model =  instantiate(cfg.model)\n",
        "    DetectionCheckpointer(model).load(model_path)\n",
        "    return model"
      ],
      "metadata": {
        "id": "MrWnFujoX_it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset paths\n",
        "rpc_main_path = '/content/SS2023_DI-Lab_Precitaste/retail_product_checkout'\n",
        "\n",
        "val_size = 0.2\n",
        "random_seed = 12"
      ],
      "metadata": {
        "id": "_4-lc1w_Yqxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = RPCDataset(rpc_main_path, 'val')\n",
        "\n",
        "sub_classes = val_dataset.get_class_names()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_sub_classes, val_sub_classes = train_test_split(sub_classes, test_size=val_size, random_state=random_seed)\n",
        "\n",
        "\n",
        "model_path =\"/content/SS2023_DI-Lab_Precitaste/model_final_866730.pkl\"\n",
        "config_path = \"/content/SS2023_DI-Lab_Precitaste/detectron2/projects/ViTDet/configs/LVIS/mask_rcnn_vitdet_h_100ep.py\"\n",
        "object_proposer_model = load_model(model_path,config_path)\n",
        "object_proposer_model.to(device)\n",
        "object_proposer_model.eval()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkLIF2KgYvI-",
        "outputId": "46f16143-9c6b-4312-ddf1-6e51d93d49f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/28 19:26:29 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /content/SS2023_DI-Lab_Precitaste/model_final_866730.pkl ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_idx = np.arange(0, val_dataset.get_num_imgs())\n",
        "train_img_idx, val_img_idx = train_test_split(img_idx, test_size=val_size, random_state=random_seed)"
      ],
      "metadata": {
        "id": "vyT_FuGWZTyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'train_sub_classes': train_sub_classes,\n",
        "    'val_sub_classes': val_sub_classes,\n",
        "    'train_img_idx': train_img_idx.tolist(),\n",
        "    'val_img_idx': val_img_idx.tolist()\n",
        "}\n",
        "with open('../train_config.json', 'w') as jfile:\n",
        "    json.dump(config, jfile)"
      ],
      "metadata": {
        "id": "G2yewYQGZogs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pred_objects(img_indices):\n",
        "    prediction_objects = []\n",
        "    post_processed_prediction_objects = []\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(img_indices):\n",
        "            element = val_dataset.get_element_by_id(i)\n",
        "            pil_img = Image.open(element['img_path'])\n",
        "            el_gt_annots = element['annots']\n",
        "            h,w = pil_img.size\n",
        "\n",
        "            img = T.PILToTensor()(pil_img.convert(\"RGB\"))\n",
        "            resized_img = T.Resize((int(h/4), int(w/4)), antialias=None)(img)\n",
        "            batch = [{'image':resized_img.to(device)}]\n",
        "            model_result=object_proposer_model(batch)\n",
        "\n",
        "            picked_boxes, picked_score = run_nms(\n",
        "                model_result[0]['instances'].pred_boxes.tensor.to(torch.int).tolist(),\n",
        "                model_result[0]['instances'].scores.tolist(),\n",
        "                np.asarray(img),\n",
        "                0.5\n",
        "            )\n",
        "            denormed_boxes = np.array(picked_boxes) * 4\n",
        "\n",
        "            remaining_boxes, remaining_boxes_indices = eliminate_boxes(denormed_boxes, h, w, area_thres=0.6, eps=10, return_bbox_indices=True)\n",
        "            remaining_scores = [picked_score[i] for i in remaining_boxes_indices]\n",
        "\n",
        "            # Extract label and bounding box of annotations and convert bboxes to xyxy format\n",
        "            el_gt_bboxes = np.array([annot[0] for annot in el_gt_annots])\n",
        "            el_gt_labels = np.array([annot[1] for annot in el_gt_annots])\n",
        "            el_gt_bboxes[:, 2] = el_gt_bboxes[:, 0] + el_gt_bboxes[:, 2]\n",
        "            el_gt_bboxes[:, 3] = el_gt_bboxes[:, 1] + el_gt_bboxes[:, 3]\n",
        "\n",
        "            corresponding_bbox_index, unmatched_bbox_index = find_gt_bboxes_of_pred(denormed_boxes, picked_score, el_gt_bboxes)\n",
        "            corresponding_post_bbox_index, unmatched_post_bbox_index = find_gt_bboxes_of_pred(remaining_boxes, remaining_scores, el_gt_bboxes)\n",
        "\n",
        "            for i, box in enumerate(denormed_boxes):\n",
        "                # Discard very small objects\n",
        "                if (box[2] - box[0]) < 56 and (box[3] - box[1]) < 56:\n",
        "                    continue\n",
        "\n",
        "                cur_object = Prediction(element['img_name'], element['img_path'], box, picked_score[i])\n",
        "\n",
        "                if corresponding_bbox_index[i] != -1:\n",
        "                    matched_gt_annot_idx = corresponding_bbox_index[i]\n",
        "                    matched_gt_annot = el_gt_annots[matched_gt_annot_idx]\n",
        "\n",
        "                    is_train_class = matched_gt_annot[1] in train_sub_classes\n",
        "\n",
        "                    cur_object.add_gt_bbox(matched_gt_annot[0], matched_gt_annot[1], is_train_class)\n",
        "\n",
        "                prediction_objects.append(cur_object)\n",
        "\n",
        "            for ind in unmatched_bbox_index:\n",
        "                not_found_bbox = el_gt_annots[ind]\n",
        "                cur_object = Prediction(element['img_name'], element['img_path'], None, None)\n",
        "\n",
        "                is_train_class = not_found_bbox[1] in train_sub_classes\n",
        "                cur_object.add_gt_bbox(not_found_bbox[0], not_found_bbox[1], is_train_class)\n",
        "\n",
        "                prediction_objects.append(cur_object)\n",
        "\n",
        "            for i, box in enumerate(remaining_boxes):\n",
        "                # Discard very small objects\n",
        "                if (box[2] - box[0]) < 56 and (box[3] - box[1]) < 56:\n",
        "                    continue\n",
        "\n",
        "                cur_object = Prediction(element['img_name'], element['img_path'], box, remaining_scores[i])\n",
        "\n",
        "                if corresponding_post_bbox_index[i] != -1:\n",
        "                    matched_gt_annot_idx = corresponding_post_bbox_index[i]\n",
        "                    matched_gt_annot = el_gt_annots[matched_gt_annot_idx]\n",
        "\n",
        "                    is_train_class = matched_gt_annot[1] in train_sub_classes\n",
        "\n",
        "                    cur_object.add_gt_bbox(matched_gt_annot[0], matched_gt_annot[1], is_train_class)\n",
        "\n",
        "                post_processed_prediction_objects.append(cur_object)\n",
        "\n",
        "            for ind in unmatched_post_bbox_index:\n",
        "                not_found_bbox = el_gt_annots[ind]\n",
        "                cur_object = Prediction(element['img_name'], element['img_path'], None, None)\n",
        "\n",
        "                is_train_class = not_found_bbox[1] in train_sub_classes\n",
        "                cur_object.add_gt_bbox(not_found_bbox[0], not_found_bbox[1], is_train_class)\n",
        "\n",
        "                post_processed_prediction_objects.append(cur_object)\n",
        "\n",
        "        return prediction_objects, post_processed_prediction_objects"
      ],
      "metadata": {
        "id": "3Fe_2i6zZt6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing with some indicies\")\n",
        "pred_objects, processed_pred_objects = get_pred_objects([58, 59])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "7e26ea38659a46f4bcf0a431aee00aec",
            "e6deab917bca4562b2c142684a8e6b08",
            "ac4eebe8ca06457d98fbe654881435b8",
            "b88cf25573e9479c8521446f635b6e33",
            "a8c16cdee83b46afb1f962f7dde7a267",
            "c692ee65ce5446ef90e413b95ecd9b0b",
            "825bdb4794454af78135a8020e9caedf",
            "fdbf4718d2f74f71b292cc08ab46d40a",
            "d80fcc66a2914ca1b89d18cd6143b1b9",
            "96245cf32a3a465aacd71f0e286aca05",
            "c8b9daab63884f5fb4d05c732d8b0f8c"
          ]
        },
        "id": "fjCEXOxDZ5kN",
        "outputId": "11969321-f77e-4e40-df37-92dc2be48d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e26ea38659a46f4bcf0a431aee00aec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_objects, processed_pred_objects = get_pred_objects(train_img_idx)\n",
        "dump_pred_objects(pred_objects, \"../vith_res_train_pred_objects.json\")\n",
        "dump_pred_objects(processed_pred_objects, \"../vith_res_train_processed_pred_objects.json\")"
      ],
      "metadata": {
        "id": "JqvhYJBTaFNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_objects, processed_pred_objects = get_pred_objects(val_img_idx)\n",
        "dump_pred_objects(pred_objects, \"../vith_res_val_pred_objects.json\")\n",
        "dump_pred_objects(processed_pred_objects, \"../vith_res_val_processed_pred_objects.json\")"
      ],
      "metadata": {
        "id": "ww8g2_j0aGYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2nd Stage: DINOv2 Feature Extraction\n",
        "\n",
        "This stage extract features for all the bounding boxes found in the first stage.\n",
        "\n",
        "Input:  Prediction objects generated in the first stage\n",
        "\n",
        "Output: Prediction objects from the first stage extended with the features extracted with Dino for all the predicted bounding box"
      ],
      "metadata": {
        "id": "YG7QJsmqKQVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "9ggahfdYih5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from notebooks.utils.dino_v2 import crop_object_with_bbox"
      ],
      "metadata": {
        "id": "XN_X-339inIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset paths\n",
        "rpc_main_path = '/content/SS2023_DI-Lab_Precitaste/retail_product_checkout'\n",
        "# Dimension of the feature vector obtained from DINO\n",
        "FEATURE_DIM = 384"
      ],
      "metadata": {
        "id": "VZWWW73tipVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset.RPCDataset import RPCDataset\n",
        "val_dataset = RPCDataset(rpc_main_path, 'val')\n",
        "sub_classes = val_dataset.get_class_names()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "zH6STr_FiyKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/train_config.json', 'r') as jfile:\n",
        "    train_config = json.load(jfile)"
      ],
      "metadata": {
        "id": "F2TAgG3Si1Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from notebooks.utils.Prediction import Prediction, get_pred_objects_per_image, read_pred_objects_json, dump_pred_objects"
      ],
      "metadata": {
        "id": "c3J6zMxbi2sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.DINO import DINOFeatureExtractor\n",
        "feat_extractor = DINOFeatureExtractor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71H3yz3_i-bk",
        "outputId": "9cea84c4-054c-48b3-8436-40dba97b1608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "WARNING:dinov2:xFormers not available\n",
            "WARNING:dinov2:xFormers not available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_feature_vector_for_bbox(pred_objects):\n",
        "    pred_objects_per_image = get_pred_objects_per_image(pred_objects)\n",
        "    processed_pred_objects = []\n",
        "    with torch.no_grad():\n",
        "        for img_name in tqdm(pred_objects_per_image):\n",
        "\n",
        "            img_pred_objects = pred_objects_per_image[img_name]\n",
        "            img_path = img_pred_objects[0].img_path\n",
        "\n",
        "            pil_img = Image.open(img_path)\n",
        "            np_img = np.asarray(pil_img)\n",
        "\n",
        "            for pred_object in img_pred_objects:\n",
        "                # Even there is no prediction for that still add the object\n",
        "                if pred_object.pred_bbox is None:\n",
        "                    processed_pred_objects.append(pred_object)\n",
        "                    continue\n",
        "\n",
        "                # The smallest gt object has a dimension of 50\n",
        "                if (pred_object.pred_bbox[2] - pred_object.pred_bbox[0]) < 50 or (pred_object.pred_bbox[3] - pred_object.pred_bbox[1]) < 50:\n",
        "                    continue\n",
        "\n",
        "                cropped_object_np_img = crop_object_with_bbox(np_img, pred_object.pred_bbox)\n",
        "                h,w,_ = cropped_object_np_img.shape\n",
        "\n",
        "                sample_feature = feat_extractor.predict(cropped_object_np_img)\n",
        "                sample_feature = sample_feature.cpu().numpy()\n",
        "\n",
        "                pred_object.add_feature_vector(sample_feature)\n",
        "                processed_pred_objects.append(pred_object)\n",
        "    return processed_pred_objects"
      ],
      "metadata": {
        "id": "hpv4JWL3jBVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_objects = read_pred_objects_json(\"/content/vith_res_train_pred_objects.json\")\n",
        "feature_extracted_pred_objects = find_feature_vector_for_bbox(pred_objects)\n",
        "dump_pred_objects(feature_extracted_pred_objects, \"../vith_res_train_pred_objects_2.json\")\n",
        "\n",
        "pred_objects = read_pred_objects_json(\"/content/vith_res_train_processed_pred_objects.json\")\n",
        "feature_extracted_pred_objects = find_feature_vector_for_bbox(pred_objects)\n",
        "dump_pred_objects(feature_extracted_pred_objects, \"../vith_res_train_processed_pred_objects_2.json\")\n",
        "\n",
        "pred_objects = read_pred_objects_json(\"/content/vith_res_val_pred_objects.json\")\n",
        "feature_extracted_pred_objects = find_feature_vector_for_bbox(pred_objects)\n",
        "dump_pred_objects(feature_extracted_pred_objects, \"../vith_res_val_pred_objects_2.json\")\n",
        "\n",
        "pred_objects = read_pred_objects_json(\"/content/vith_res_val_processed_pred_objects.json\")\n",
        "feature_extracted_pred_objects = find_feature_vector_for_bbox(pred_objects)\n",
        "dump_pred_objects(feature_extracted_pred_objects, \"../vith_res_val_processed_pred_objects_2.json\")"
      ],
      "metadata": {
        "id": "b2A-1tN4jLQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Stage: Extracting Training Features\n",
        "\n",
        "This stage extracts Dino features of the ground truth bounding boxes and is required for the next stage: bounding box prediction.\n"
      ],
      "metadata": {
        "id": "yU2kI8xwPhEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/uonat/SS2023_DI-Lab_Precitaste.git"
      ],
      "metadata": {
        "id": "EwDFW9GPRsre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/SS2023_DI-Lab_Precitaste'"
      ],
      "metadata": {
        "id": "2wK7CM2pSKku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import sys\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "from dataset.RPCDataset import RPCDataset\n",
        "from notebooks.utils.dino_v2 import crop_object_with_bbox\n",
        "from models.DINO import DINOFeatureExtractor"
      ],
      "metadata": {
        "id": "dKMZAw76QfLq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_names = os.listdir('/kaggle/input/retail-product-checkout-dataset/train2019')\n",
        "train_dataset = RPCDataset('/kaggle/input/retail-product-checkout-dataset', 'train')"
      ],
      "metadata": {
        "id": "jBO03ah3QgC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = RPCDataset('/kaggle/input/retail-product-checkout-dataset', 'val')\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "20OgHDbYQkOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = DINOFeatureExtractor()\n",
        "feature_vector_parent_folder_path = 'rpc-train-efficientnetv2-feat'\n",
        "object_img_parent_folder_path = 'rpc-train-efficientnetv2-imgs'"
      ],
      "metadata": {
        "id": "Vq70J6ASSRCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for i in tqdm(range(train_dataset.get_num_imgs())):\n",
        "        img_name = train_dataset.get_img_name_by_id(i)\n",
        "\n",
        "        element = train_dataset.get_element_by_id(i)\n",
        "\n",
        "        # Get object from bbox and make it compatible with dinov2\n",
        "        bbox = element['annots'][0][0]\n",
        "        object_name = element['annots'][0][1]\n",
        "        np_cropped_object = crop_object_with_bbox(element['img'], bbox)\n",
        "\n",
        "        feats = feature_extractor.predict(np_cropped_object)\n",
        "\n",
        "        object_feature_folder_path = os.path.join(feature_vector_parent_folder_path, object_name)\n",
        "        os.makedirs(object_feature_folder_path, exist_ok=True)\n",
        "        npy_name = img_name.replace('jpg', '')\n",
        "        object_feature_file_path = os.path.join(object_feature_folder_path, '{}.npy'.format(npy_name))\n",
        "\n",
        "        with open(object_feature_file_path, 'wb') as npfile:\n",
        "            np.save(npfile, feats.cpu().numpy(), allow_pickle=True)"
      ],
      "metadata": {
        "id": "hyG5Q73KShiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3rd Stage: Bounding Box Label Assignment\n",
        "\n",
        "Final stage of the algorithm makes prediction for the bounding box found till this point. Predictions are made by finding the label of the most similar item in the dataset. To find the most similar item in the dataset, this stage requires training object features extracted in the same way with the second stage using the ground truth boxes.\n",
        "\n",
        "\n",
        "Along with the predictions this stage also generates some\n",
        "\n",
        "\n",
        "Input: Prediction objects from the previous stage. Dino features of the training objects.\n",
        "\n",
        "Output: mAP score of the algorithm"
      ],
      "metadata": {
        "id": "M61oa-deLmx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.patches as mpatches\n",
        "import cv2\n",
        "\n",
        "from dataset.utils import select_random, select_uniform, select_per_cam, load_npy_files\n",
        "from notebooks.utils.Prediction import get_pred_objects_per_image, read_pred_objects_json, dump_pred_objects"
      ],
      "metadata": {
        "id": "akyx5xLSnEaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_features_path = '/kaggle/input/idp-repo/rpc-train-dino-feat-full-img/rpc-train-dino-feat-full-img/rpc-train-dino-feat-full-img'\n",
        "rpc_val_set_path = '/kaggle/input/retail-product-checkout-dataset/val2019/'\n",
        "\n",
        "class_names = [fname for fname in os.listdir(training_features_path)]"
      ],
      "metadata": {
        "id": "PzBrFKVdnJiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many samples to select from each train class\n",
        "N_SAMPLES = 1\n",
        "# How to select training samples\n",
        "# n_per_cam, uniform\n",
        "SELECT_METHOD = 'random'\n",
        "RANDOM_SEED = 24\n",
        "VAL_SIZE = 0.4\n",
        "# How to divide validation classes\n",
        "# base, vanilla\n",
        "# base will divide main classes and assigns fine labels to parents\n",
        "# vanilla will divide directly\n",
        "# In base it is not possible to have 11_tissue in train and 12_tissue in val\n",
        "SPLIT_METHOD = 'base'\n",
        "# Dimension of the extracted features\n",
        "FEATURE_DIM = 384"
      ],
      "metadata": {
        "id": "FaScQsv0nMJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_split(split_method):\n",
        "    if split_method == 'vanilla':\n",
        "        train_classes, val_classes = train_test_split(class_names, test_size=VAL_SIZE, random_state=RANDOM_SEED)\n",
        "    elif split_method == 'base':\n",
        "        base_class_names = list(set([''.join(cname.split('_')[1:]) for cname in class_names]))\n",
        "        train_base_classes, val_base_classes = train_test_split(base_class_names, test_size=VAL_SIZE, random_state=RANDOM_SEED)\n",
        "        train_classes = [cname for cname in class_names if ''.join(cname.split('_')[1:]) in train_base_classes]\n",
        "        val_classes = [cname for cname in class_names if ''.join(cname.split('_')[1:]) in val_base_classes]\n",
        "    return train_classes, val_classes"
      ],
      "metadata": {
        "id": "MmUiVq2HoguV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILD_DATASETS\n",
        "# Read feature vectors by the algorithm specified\n",
        "def build_datasets(select_method, n_samples, tr_classes):\n",
        "    train_features = []\n",
        "    val_features = []\n",
        "\n",
        "    train_labels = []\n",
        "    val_labels = []\n",
        "\n",
        "    for class_name in tqdm(class_names):\n",
        "        class_folder_path = os.path.join(training_features_path, class_name)\n",
        "        img_names = [fname for fname in os.listdir(class_folder_path) if '.npy' in fname]\n",
        "\n",
        "        if select_method == 'n_per_cam':\n",
        "            selected_img_names = select_per_cam(img_names, n_samples)\n",
        "        elif select_method == 'random':\n",
        "            selected_img_names = select_random(img_names, n_samples)\n",
        "        elif select_method == 'uniform':\n",
        "            selected_img_names = select_uniform(img_names, n_samples)\n",
        "\n",
        "        folder_features = load_npy_files(class_folder_path, selected_img_names)\n",
        "        if class_name in tr_classes:\n",
        "            train_features.append(folder_features)\n",
        "            train_labels = train_labels + [class_name] * len(folder_features)\n",
        "        else:\n",
        "            val_features.append(folder_features)\n",
        "            val_labels = val_labels + [class_name] * len(folder_features)\n",
        "\n",
        "    np_train_features = np.array(train_features).reshape(-1, FEATURE_DIM)\n",
        "    np_train_labels = np.array(train_labels)\n",
        "\n",
        "    np_val_features = np.array(val_features).reshape(-1, FEATURE_DIM)\n",
        "    np_val_labels = np.array(val_labels)\n",
        "    return np_train_features, np_train_labels, np_val_features, np_val_labels"
      ],
      "metadata": {
        "id": "2C_fGk_Fol_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finds the most similar feature vector and similarity to that\n",
        "# in the training set for a prediction object created in the previous steps\n",
        "def predict_bboxes(pred_objects, tr_class_names, tr_features, tr_labels, val_features, val_labels):\n",
        "    train_pred_objects = []\n",
        "    val_pred_objects = []\n",
        "\n",
        "    for pred_objects in tqdm(pred_objects):\n",
        "        if pred_objects.pred_bbox is not None:\n",
        "            if pred_objects.gt_label is not None:\n",
        "                # Do matching, assign to train\n",
        "                if pred_objects.gt_label in tr_class_names:\n",
        "                    similarities = cosine_similarity(np.array([pred_objects.pred_features]).reshape(1,-1), tr_features)\n",
        "                    best_sim_index = np.argmax(similarities)\n",
        "                    score = similarities[0, best_sim_index]\n",
        "                    label = tr_labels[best_sim_index]\n",
        "                    pred_objects.add_classification_res(label, score)\n",
        "                    train_pred_objects.append(pred_objects)\n",
        "                # Do matching, assign to val\n",
        "                else:\n",
        "                    similarities = cosine_similarity(np.array([pred_objects.pred_features]).reshape(1,-1), val_features)\n",
        "                    best_sim_index = np.argmax(similarities)\n",
        "                    score = similarities[0, best_sim_index]\n",
        "                    label = val_labels[best_sim_index]\n",
        "                    pred_objects.add_classification_res(label, score)\n",
        "                    val_pred_objects.append(pred_objects)\n",
        "            # No gt label -> FP\n",
        "            # Measure dist to train, try to find a threshold\n",
        "            else:\n",
        "                similarities = cosine_similarity(np.array([pred_objects.pred_features]).reshape(1,-1), tr_features)\n",
        "                best_sim_index = np.argmax(similarities)\n",
        "                score = similarities[0, best_sim_index]\n",
        "                label = tr_labels[best_sim_index]\n",
        "                pred_objects.add_classification_res(label, score)\n",
        "                train_pred_objects.append(pred_objects)\n",
        "        # Missed pred nothing to do add train or val\n",
        "        else:\n",
        "            if pred_objects.gt_label in tr_class_names:\n",
        "                train_pred_objects.append(pred_objects)\n",
        "            else:\n",
        "                val_pred_objects.append(pred_objects)\n",
        "\n",
        "    return train_pred_objects, val_pred_objects"
      ],
      "metadata": {
        "id": "5I3Uah69ooUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finds similarity distribution between elements of the same class and\n",
        "# different class for the train split\n",
        "def find_similarity_dist_train(tr_labels, tr_features):\n",
        "\n",
        "    all_inner_class_sims = []\n",
        "    all_inter_class_sims = []\n",
        "    unique_train_labels = np.unique(tr_labels)\n",
        "\n",
        "    for train_label in unique_train_labels:\n",
        "        label_mask = np.where(np.array(tr_labels) == train_label)\n",
        "        not_label_mask = np.where(np.array(tr_labels) != train_label)\n",
        "\n",
        "        label_features = tr_features[label_mask]\n",
        "        inner_similarity = cosine_similarity(label_features, label_features)\n",
        "        inter_similarity = cosine_similarity(label_features, tr_features[not_label_mask])\n",
        "        np.fill_diagonal(inner_similarity, -1)\n",
        "\n",
        "        all_inner_class_sims += inner_similarity.flatten().tolist()\n",
        "        all_inter_class_sims += inter_similarity.flatten().tolist()\n",
        "    return all_inner_class_sims, all_inter_class_sims"
      ],
      "metadata": {
        "id": "82C8wNFToqbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sim_dist_pred_object(pred_objects, tr_features, tr_labels):\n",
        "\n",
        "    all_inner_class_sims = []\n",
        "    all_inter_class_sims = []\n",
        "    fp_sims = []\n",
        "\n",
        "    for pred_object in tqdm(pred_objects):\n",
        "\n",
        "        if pred_object.gt_label is not None and pred_object.gt_label in tr_labels and pred_object.pred_features is not None:\n",
        "\n",
        "            train_label = pred_object.gt_label\n",
        "            label_mask = np.where(np.array(tr_labels) == train_label)\n",
        "            not_label_mask = np.where(np.array(tr_labels) != train_label)\n",
        "\n",
        "            pred_object_features = np.array(pred_object.pred_features).reshape(1, -1)\n",
        "            label_features = tr_features[label_mask]\n",
        "            no_label_features = tr_features[not_label_mask]\n",
        "\n",
        "            inner_similarity = cosine_similarity(pred_object_features, label_features)\n",
        "            inter_similarity = cosine_similarity(pred_object_features, no_label_features)\n",
        "\n",
        "            all_inner_class_sims += inner_similarity.flatten().tolist()\n",
        "            all_inter_class_sims += inter_similarity.flatten().tolist()\n",
        "\n",
        "        elif pred_object.pred_features is not None and pred_object.gt_label is None:\n",
        "\n",
        "            pred_object_features = np.array(pred_object.pred_features).reshape(1, -1)\n",
        "            fp_similarity = cosine_similarity(pred_object_features, tr_features)\n",
        "            fp_sims += fp_similarity.flatten().tolist()\n",
        "\n",
        "    return all_inner_class_sims, all_inter_class_sims, fp_sims"
      ],
      "metadata": {
        "id": "wwFGDDWCosGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dist(title, dist1, label1, dist2=None, label2=None):\n",
        "    labels = []\n",
        "\n",
        "    plt.figure(figsize=(9,6))\n",
        "\n",
        "    violin = plt.violinplot(dist1)\n",
        "\n",
        "    color = violin[\"bodies\"][0].get_facecolor().flatten()\n",
        "    labels.append((mpatches.Patch(color=color), label1))\n",
        "\n",
        "    if dist2 is not None:\n",
        "        violin2 = plt.violinplot(dist2)\n",
        "        color = violin2[\"bodies\"][0].get_facecolor().flatten()\n",
        "        labels.append((mpatches.Patch(color=color), label2))\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.legend(*zip(*labels), loc=2)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bFQOqF8qouDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_pred_objects(per_image_objects, th):\n",
        "    all_image_stats = []\n",
        "    for img_name in per_image_objects:\n",
        "        image_stats = {'img_name': img_name, 'TH': th, 'FP': 0, 'TP': 0, 'FN': 0, 'Precision': 0.0, 'Recall': 0.0}\n",
        "        image_pred_objects = per_image_objects[img_name]\n",
        "\n",
        "        for pred_objects in image_pred_objects:\n",
        "            if (pred_objects.pred_bbox is not None) and (pred_objects.class_score > th) and pred_objects.pred_label is not None:\n",
        "                # Bounding box is found extra by the detection network\n",
        "                if pred_objects.gt_bbox is None:\n",
        "                    image_stats['FP'] += 1\n",
        "                # Bounding box misclassified by the knn\n",
        "                elif pred_objects.gt_label != pred_objects.pred_label:\n",
        "                    image_stats['FP'] += 1\n",
        "                # True detection\n",
        "                elif pred_objects.gt_label == pred_objects.pred_label:\n",
        "                    image_stats['TP'] += 1\n",
        "\n",
        "            # Bounding box is missed by the detection network\n",
        "            elif pred_objects.gt_label is not None:\n",
        "                image_stats['FN'] += 1\n",
        "\n",
        "        if (image_stats['TP'] + image_stats['FP']) > 0:\n",
        "            image_stats['Precision'] = image_stats['TP'] / (image_stats['TP'] + image_stats['FP'])\n",
        "        else:\n",
        "            image_stats['Precision'] = None\n",
        "\n",
        "        if (image_stats['TP'] + image_stats['FN']) > 0:\n",
        "            image_stats['Recall'] = image_stats['TP'] / (image_stats['TP'] + image_stats['FN'])\n",
        "        else:\n",
        "            image_stats['Recall'] = None\n",
        "\n",
        "        all_image_stats.append(image_stats)\n",
        "    return all_image_stats\n",
        "\n",
        "def eval_with_diff_th(per_image_objects):\n",
        "    all_image_stats = []\n",
        "    for th in np.linspace(0, 0.95, 20):\n",
        "        all_image_stats += eval_pred_objects(per_image_objects, th)\n",
        "    return all_image_stats"
      ],
      "metadata": {
        "id": "yNL0sDVBo3-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_per_img_pred_objects_with_th(per_image_pred_objects, gt_path, pred_path, bounding_box_th, class_score_th, use_base_classes=False):\n",
        "\n",
        "    os.makedirs(gt_path, exist_ok=True)\n",
        "    os.makedirs(pred_path, exist_ok=True)\n",
        "\n",
        "    for img_name in per_image_pred_objects:\n",
        "        txt_name = img_name.split('.')[0] + '.txt'\n",
        "        pred_txt_dir = os.path.join(pred_path, txt_name)\n",
        "        with open(pred_txt_dir, \"w\") as txtfile:\n",
        "            for predict_object in per_image_pred_objects[img_name]:\n",
        "\n",
        "                if predict_object.pred_bbox is not None and predict_object.pred_score_bbox > bounding_box_th and (predict_object.class_score is not None and predict_object.class_score > class_score_th):\n",
        "                    label = predict_object.pred_label\n",
        "                    if use_base_classes:\n",
        "                        label = ''.join(label.split('_')[1:])\n",
        "                    x1, y1, x2, y2 = predict_object.pred_bbox\n",
        "                    conf = predict_object.class_score\n",
        "                    txtfile.write(\"{} {} {} {} {} {}\\n\".format(label, conf, x1, y1, x2-x1, y2-y1))\n",
        "\n",
        "        gt_txt_dir = os.path.join(gt_path, txt_name)\n",
        "        with open(gt_txt_dir, \"w\") as txtfile:\n",
        "            for predict_object in per_image_pred_objects[img_name]:\n",
        "\n",
        "                if predict_object.gt_bbox is not None:\n",
        "                    label = predict_object.gt_label\n",
        "                    if use_base_classes:\n",
        "                        label = ''.join(label.split('_')[1:])\n",
        "                    x1, y1, w, h = predict_object.gt_bbox\n",
        "                    # GT bbox is in the form of xywh\n",
        "                    txtfile.write(\"{} {} {} {} {}\\n\".format(label, x1, y1, w, h))"
      ],
      "metadata": {
        "id": "Mcju5kqbo7WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_eval_script(gt_folder_path, pred_folder_path, save_folder_path):\n",
        "    script = \"python evaluation/object-detection-metrics/evaluate.py -gt {} -det {} -sp {}\".format(gt_folder_path, pred_folder_path, save_folder_path)\n",
        "    print(script)\n",
        "    os.system(script)\n",
        "\n",
        "def print_mAP(eval_res_path):\n",
        "    metrics_df_path = os.path.join(eval_res_path, 'eval_results_per_image.csv')\n",
        "    results_df = pd.read_csv(metrics_df_path)\n",
        "    groupped_df = results_df.groupby(['class']).mean(numeric_only=True)\n",
        "    print(groupped_df.mean())"
      ],
      "metadata": {
        "id": "Rq9qYOsXo9B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bboxes_from_txt(txtfile_path):\n",
        "    bboxes = []\n",
        "    with open(txtfile_path, 'r') as txtfile:\n",
        "        lines = txtfile.readlines()\n",
        "        for line in lines:\n",
        "            splitted_lines = line.split(\" \")\n",
        "            bboxes.append([float(i) for i in splitted_lines[-4:]] + [splitted_lines[0]])\n",
        "    return bboxes\n",
        "\n",
        "def draw_bboxes(img_path, gt_boxes, pred_boxes, train_classes):\n",
        "    # Draw parameters\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2\n",
        "    thickness = 2\n",
        "    text_color = (113, 24, 200)\n",
        "    tr_bbox_color = (12, 154, 242)\n",
        "    val_bbox_color = (164, 12, 242)\n",
        "\n",
        "    org_img = cv2.imread(img_path)\n",
        "    org_img = cv2.cvtColor(org_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    gt_drawn_img = org_img.copy()\n",
        "    pred_drawn_img = org_img.copy()\n",
        "\n",
        "    for bbox in gt_boxes:\n",
        "        start_x, start_y, bbox_w, bbox_h = [int(i) for i in bbox[:4]]\n",
        "        label = bbox[-1]\n",
        "        if label in train_classes:\n",
        "            bbox_color = tr_bbox_color\n",
        "        else:\n",
        "            bbox_color = val_bbox_color\n",
        "        cv2.putText(gt_drawn_img, label, (start_x, start_y-10), font, font_scale, text_color, thickness)\n",
        "        cv2.rectangle(gt_drawn_img, (start_x, start_y), (start_x + bbox_w, start_y+bbox_h), bbox_color, 4)\n",
        "\n",
        "    for bbox in pred_boxes:\n",
        "        start_x, start_y, bbox_w, bbox_h = [int(i) for i in bbox[:4]]\n",
        "        label = bbox[-1]\n",
        "        if label in train_classes:\n",
        "            bbox_color = tr_bbox_color\n",
        "        else:\n",
        "            bbox_color = val_bbox_color\n",
        "        cv2.putText(pred_drawn_img, label, (start_x, start_y-10), font, font_scale, text_color, thickness)\n",
        "        cv2.rectangle(pred_drawn_img, (start_x, start_y), (start_x + bbox_w, start_y+bbox_h), bbox_color, 4)\n",
        "\n",
        "    return gt_drawn_img, pred_drawn_img\n",
        "\n",
        "def plot_and_save_fig(gt_img, pred_img, fig_save_path):\n",
        "\n",
        "    fig, axis = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    axis[0].imshow(gt_img)\n",
        "    axis[0].set_title('GT')\n",
        "    axis[0].set_xticks([])\n",
        "    axis[0].set_yticks([])\n",
        "\n",
        "    axis[1].imshow(pred_img)\n",
        "    axis[1].set_title('Pred')\n",
        "    axis[1].set_xticks([])\n",
        "    axis[1].set_yticks([])\n",
        "\n",
        "    plt.savefig(fig_save_path)\n",
        "    plt.close()\n",
        "\n",
        "def visualize_samples(gt_annots_path, pred_annots_path, eval_res_path, fig_save_path, train_classes):\n",
        "\n",
        "    results_csv_path = os.path.join(eval_res_path, 'eval_results_per_image.csv')\n",
        "    results_df = pd.read_csv(results_csv_path)\n",
        "\n",
        "    filtered_results_df = results_df[~results_df['AP'].isna()]\n",
        "    ap_sorted_df = filtered_results_df.sort_values(by=['Precision', 'Recall'])\n",
        "    os.makedirs(fig_save_path, exist_ok=True)\n",
        "\n",
        "    for start in [0, len(results_df) // 2, len(results_df) - 12]:\n",
        "        end = start + 10\n",
        "        for i, row in ap_sorted_df[start:end].iterrows():\n",
        "            img_name = row['img_name']\n",
        "            txt_name = img_name + '.txt'\n",
        "\n",
        "            pred_txt_path = os.path.join(pred_annots_path, txt_name)\n",
        "            gt_txt_path = os.path.join(gt_annots_path, txt_name)\n",
        "\n",
        "            pred_boxes = get_bboxes_from_txt(pred_txt_path)\n",
        "            gt_boxes = get_bboxes_from_txt(gt_txt_path)\n",
        "\n",
        "            img_path = os.path.join('/kaggle/input/retail-product-checkout-dataset/val2019/', img_name + '.jpg')\n",
        "\n",
        "            gt_img, pred_img = draw_bboxes(img_path, gt_boxes, pred_boxes, train_classes)\n",
        "            fig_img_save_path = os.path.join(fig_save_path, \"fig_{}.png\".format(i))\n",
        "            plot_and_save_fig(gt_img, pred_img, fig_img_save_path)"
      ],
      "metadata": {
        "id": "bp519WoipAWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_algo(select_method, n_samples, directory_prefix):\n",
        "    train_classes, val_classes = make_split(SPLIT_METHOD)\n",
        "    np_train_features, np_train_labels, np_val_features, np_val_labels = build_datasets(select_method,\n",
        "                                                                                        n_samples,\n",
        "                                                                                        train_classes)\n",
        "\n",
        "    print(\"Training set with size: {} label size: {}\".format(np_train_features.shape, np_train_labels.shape))\n",
        "    print(\"Val set with size: {} label size: {}\".format(np_val_features.shape, np_val_labels.shape))\n",
        "\n",
        "    train_pred_objects = read_pred_objects_json(\"/kaggle/input/di-lab-idea2-artifacts/Dino-Full-Img/vith_res_train_processed_pred_objects_full_img_2.json\")\n",
        "    val_pred_objects = read_pred_objects_json(\"/kaggle/input/di-lab-idea2-artifacts/Dino-Full-Img/vith_res_val_processed_pred_objects_full_img_2.json\")\n",
        "\n",
        "    all_pred_objects = train_pred_objects + val_pred_objects\n",
        "\n",
        "    print(\"Loaded prediction objects\")\n",
        "    # Make a prediction for all prediction objects, then divide them by classes\n",
        "    train_pred_objects, val_pred_objects = predict_bboxes(all_pred_objects, train_classes, np_train_features,\n",
        "                                                          np_train_labels, np_val_features, np_val_labels)\n",
        "    print(\"Prediction completed\")\n",
        "    # For each object in train set(train split) of RPC, finds its similarity with other elements in the same subset\n",
        "    #tr_inner_sim, tr_inter_sim = find_similarity_dist_train(np_train_labels, np_train_features)\n",
        "\n",
        "    #plot_dist(\"Training set similarities\", [i for i in tr_inner_sim if i != -1],\n",
        "    #          \"Same class\", tr_inter_sim, \"Diff class\")\n",
        "\n",
        "    # For each train object in validation split, find similarity to train classes and fn\n",
        "    tr_pred_object_inner_sim, tr_pred_object_inter_sim, fn_sim = find_sim_dist_pred_object(train_pred_objects, np_train_features, np_train_labels)\n",
        "\n",
        "    sample_to_vis = len(tr_pred_object_inner_sim)\n",
        "    subset_dist_2 = np.random.choice(tr_pred_object_inter_sim, size=sample_to_vis)\n",
        "    plot_dist(\"Train split similarity dist\", tr_pred_object_inner_sim, \"Same\", subset_dist_2, \"Diff\")\n",
        "    subset_dist3 = np.random.choice(fn_sim, size=sample_to_vis)\n",
        "    plot_dist(\"Train split FN similarities\", subset_dist3, \"Similarities\")\n",
        "\n",
        "    # Group pred objects by image name for metric calculations\n",
        "    train_per_image_objects = get_pred_objects_per_image(train_pred_objects)\n",
        "\n",
        "    # Find metrics for changing th in the training subset\n",
        "    tr_image_stats = eval_with_diff_th(train_per_image_objects)\n",
        "    metric_df = pd.DataFrame(tr_image_stats)\n",
        "\n",
        "    # Calculate precision and recall on different thresholds and visualise the metrics\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    for th in np.linspace(0.0, 0.95, 20):\n",
        "        filtered_df = metric_df[metric_df['TH'] == th]\n",
        "        mean_precision = filtered_df['Precision'].mean()\n",
        "        mean_recall = filtered_df['Recall'].mean()\n",
        "        precisions.append(mean_precision)\n",
        "        recalls.append(mean_recall)\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.xticks(ticks=np.arange(0, 20), labels=[np.round(i,2) for i in np.linspace(0.0, 0.95, 20)])\n",
        "    plt.plot(precisions, label='Precision')\n",
        "    plt.plot(recalls, label='Recall')\n",
        "    plt.title('Precision-Recall on bbox score')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Group pred objects by image name for evaluating\n",
        "    val_per_image_objects = get_pred_objects_per_image(val_pred_objects)\n",
        "    val_image_stats = eval_pred_objects(val_per_image_objects, 0.4)\n",
        "\n",
        "    metric_df = pd.DataFrame(val_image_stats)\n",
        "    mean_precision = metric_df['Precision'].mean()\n",
        "    mean_recall = metric_df['Recall'].mean()\n",
        "    print(\"Val Set: Precision: {:2f}, Recall: {:2f}\".format(mean_precision, mean_recall))\n",
        "\n",
        "    # Export training preds\n",
        "    gt_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'tr_gt_annot_full/')\n",
        "    pred_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'tr_pred_annot_full/')\n",
        "    res_save_dir = os.path.join('/kaggle/working/', directory_prefix, 'tr_pred_res_full/')\n",
        "\n",
        "    export_per_img_pred_objects_with_th(train_per_image_objects, gt_annot_dir, pred_annot_dir, 0.2, 0.4, False)\n",
        "    run_eval_script(gt_annot_dir, pred_annot_dir, res_save_dir)\n",
        "    print_mAP(res_save_dir)\n",
        "\n",
        "    gt_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'tr_gt_annot_base/')\n",
        "    pred_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'tr_pred_annot_base/')\n",
        "    res_save_dir = os.path.join('/kaggle/working/', directory_prefix, 'tr_pred_res_base/')\n",
        "\n",
        "    export_per_img_pred_objects_with_th(train_per_image_objects, gt_annot_dir, pred_annot_dir, 0.2, 0.4, True)\n",
        "    run_eval_script(gt_annot_dir, pred_annot_dir, res_save_dir)\n",
        "    print_mAP(res_save_dir)\n",
        "\n",
        "    # Export validation preds\n",
        "    gt_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'val_gt_annot_full/')\n",
        "    pred_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'val_pred_annot_full/')\n",
        "    res_save_dir = os.path.join('/kaggle/working/', directory_prefix, 'val_pred_res_full/')\n",
        "    fig_save_dir = os.path.join('/kaggle/working/', directory_prefix, 'val_pred_figures_full/')\n",
        "\n",
        "    export_per_img_pred_objects_with_th(val_per_image_objects, gt_annot_dir, pred_annot_dir, 0.2, 0.4, False)\n",
        "    run_eval_script(gt_annot_dir, pred_annot_dir, res_save_dir)\n",
        "    print_mAP(res_save_dir)\n",
        "\n",
        "    gt_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'val_gt_annot_base/')\n",
        "    pred_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'val_pred_annot_base/')\n",
        "    res_save_dir = os.path.join('/kaggle/working/', directory_prefix, 'val_pred_res_base/')\n",
        "    fig_save_dir = os.path.join('/kaggle/working/', directory_prefix, 'val_pred_figures_base/')\n",
        "\n",
        "    export_per_img_pred_objects_with_th(val_per_image_objects, gt_annot_dir, pred_annot_dir, 0.2, 0.4, True)\n",
        "    run_eval_script(gt_annot_dir, pred_annot_dir, res_save_dir)\n",
        "    print_mAP(res_save_dir)\n",
        "\n",
        "    # Export again to visualize this time export all\n",
        "    all_per_image_objects = get_pred_objects_per_image(val_pred_objects + train_pred_objects)\n",
        "\n",
        "    gt_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'all_gt_annot_full/')\n",
        "    pred_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'all_pred_annot_full/')\n",
        "    res_save_dir = os.path.join('/kaggle/working/', directory_prefix, 'all_pred_res_full/')\n",
        "    fig_save_dir = os.path.join('/kaggle/working/', directory_prefix, 'all_pred_figures_full/')\n",
        "\n",
        "    export_per_img_pred_objects_with_th(all_per_image_objects, gt_annot_dir, pred_annot_dir, 0.2, 0.4, False)\n",
        "    run_eval_script(gt_annot_dir, pred_annot_dir, res_save_dir)\n",
        "    print_mAP(res_save_dir)\n",
        "\n",
        "    visualize_samples(gt_annot_dir, pred_annot_dir, res_save_dir, fig_save_dir, train_classes)\n",
        "\n",
        "    gt_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'all_gt_annot_base/')\n",
        "    pred_annot_dir = os.path.join('/kaggle/working/', directory_prefix, 'all_pred_annot_base/')\n",
        "    res_save_dir = os.path.join('/kaggle/working/', directory_prefix, 'all_pred_res_base/')\n",
        "    fig_save_dir = os.path.join('/kaggle/working/', directory_prefix, 'all_pred_figures_base/')\n",
        "\n",
        "    export_per_img_pred_objects_with_th(all_per_image_objects, gt_annot_dir, pred_annot_dir, 0.2, 0.4, True)\n",
        "    run_eval_script(gt_annot_dir, pred_annot_dir, res_save_dir)\n",
        "    print_mAP(res_save_dir)\n",
        "\n",
        "    visualize_samples(gt_annot_dir, pred_annot_dir, res_save_dir, fig_save_dir, train_classes)"
      ],
      "metadata": {
        "id": "68ExuGZLpEBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_algo('random', 1, \"setup_1\")"
      ],
      "metadata": {
        "id": "CGUAzTr2HwD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}