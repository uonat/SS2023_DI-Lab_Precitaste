{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/uonat/SS2023_DI-Lab_Precitaste.git &> /dev/null\n",
    "%cd SS2023_DI-Lab_Precitaste\n",
    "%pip install . &> /dev/null\n",
    "\n",
    "import distutils.core\n",
    "import sys,os\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'  &> /dev/null\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])} &> /dev/null\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
    "\n",
    "%pip install ftfy regex tqdm &> /dev/null\n",
    "%pip install git+https://github.com/openai/CLIP.git &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.CLIP import load_model as load_clip,Calculate_Embeddings,tokenize_text,Calculate_Scores,get_total_num_obj,available_clip_models\n",
    "#available_clip_models()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = load_clip(\"ViT-B/32\",device)\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '/content/retail_product_checkout'\n",
    "!unzip -q -j \"/content/drive/MyDrive/ApplicationProject/Data/retail-product-checkout-dataset.zip\" \"val2019/*\" -d '/content/retail_product_checkout/val2019' \n",
    "!unzip -q -j \"/content/drive/MyDrive/ApplicationProject/Data/retail-product-checkout-dataset.zip\" \"instances_val2019.json\" -d '/content/retail_product_checkout'\n",
    "!unzip -q -j \"/content/drive/MyDrive/ApplicationProject/Data/retail-product-checkout-dataset.zip\" \"train2019/*\" -d '/content/retail_product_checkout/train2019' \n",
    "!unzip -q -j \"/content/drive/MyDrive/ApplicationProject/Data/retail-product-checkout-dataset.zip\" \"instances_train2019.json\" -d '/content/retail_product_checkout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.RPCDataset import RPCDataset\n",
    "dataset_path = \"/content/retail_product_checkout\"\n",
    "val_dataset = RPCDataset(dataset_path, \"val\")\n",
    "train_dataset = RPCDataset(dataset_path, \"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the embeddings of all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_path = \"/content/drive/MyDrive/ApplicationProject/Results\"\n",
    "output_name = \"clip_gt_train_image_embs.pkl\"\n",
    "if os.path.isfile(os.path.join(Results_path,output_name)):\n",
    "  from datetime import datetime\n",
    "  tmp_str = str(datetime.now()).split(' ')\n",
    "  output_name = output_name.split('.')[0] + '_' + tmp_str[0] + '_' + tmp_str[1].split('.')[0].replace(\":\", \"-\") +\".pkl\"\n",
    "output_dir = os.path.join(Results_path,output_name)\n",
    "\n",
    "Results = Calculate_Embeddings(clip_model,preprocess,train_dataset,output_dir,device)\n",
    "\n",
    "dict_all = {}\n",
    "for res in Results:\n",
    "  try:\n",
    "    dict_all[res[1]].append(res[0])\n",
    "  except KeyError:\n",
    "    dict_all[res[1]] = [res[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-grained labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: currently averaging over all train data to get target_feature embeddings => try few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dict_finegrained_mean_feature_vecs = {}\n",
    "for key in dict_all.keys():\n",
    "  dict_finegrained_mean_feature_vecs[key] =  np.stack(dict_all[key],axis=0).mean(axis=0)\n",
    "\n",
    "#order of labels\n",
    "all_labels_finegrained = list(dict_all.keys())\n",
    "print(\"len(all_labels_finegrained):\",len(all_labels_finegrained))\n",
    "print(\"all_labels_finegrained:\",all_labels_finegrained)\n",
    "\n",
    "all_labels_broad = set() \n",
    "for i in range(val_dataset.get_num_imgs()):\n",
    "  annots  = val_dataset.get_annots_by_img_id(i, key_for_category='sku_name')\n",
    "  for annot in annots:\n",
    "    all_labels_broad.add(' '.join(annot[1].split('_')[1:]))\n",
    "all_labels_broad = list(all_labels_broad) #Broad labels\n",
    "print(\"len(all_labels_broad):\",len(all_labels_broad))\n",
    "print(\"all_labels_broad:\",all_labels_broad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = []\n",
    "for label in all_labels_finegrained:\n",
    "  target_features.append(dict_finegrained_mean_feature_vecs[label])\n",
    "target_features = torch.from_numpy(np.stack(target_features, axis=0)).type(torch.float16).to(device)\n",
    "\n",
    "Results_path = \"/content/drive/MyDrive/ApplicationProject/Results\"\n",
    "output_name = \"clip_gt_fewShot_finegrained_allTrain_Result.pkl\"\n",
    "if os.path.isfile(os.path.join(Results_path,output_name)):\n",
    "  from datetime import datetime\n",
    "  tmp_str = str(datetime.now()).split(' ')\n",
    "  output_name = output_name.split('.')[0] + '_' + tmp_str[0] + '_' + tmp_str[1].split('.')[0].replace(\":\", \"-\") +\".pkl\"\n",
    "output_dir = os.path.join(Results_path,output_name)\n",
    "\n",
    "Results = Calculate_Scores(clip_model,preprocess,val_dataset,target_features,all_labels_finegrained,output_dir,device,True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Fine-grained Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "gt_label = []\n",
    "for res in Results:\n",
    "  tmp_arr = [0] * len(all_labels_finegrained)\n",
    "  tmp_arr[res[1]] = 1\n",
    "  scores += res[0]\n",
    "  gt_label += tmp_arr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images: \",len(val_dataset.get_num_imgs())) \n",
    "print(\"Number of objects: \",get_total_num_obj(val_dataset)) \n",
    "print(\"Number of classes: \",len(all_labels_finegrained))\n",
    "assert len(scores) == len(gt_label)\n",
    "assert len(scores) == get_total_num_obj(val_dataset)*len(all_labels_finegrained)\n",
    "print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score,average_precision_score,accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Prediction Results:\")\n",
    "\n",
    "def to_labels(pos_probs, threshold):\n",
    " return [1 if nm > threshold else 0 for nm in pos_probs]\n",
    "\n",
    "cand_thresholds = [x / 100.0 for x in range(10, 95, 5)]\n",
    "f1_scores_for_thrs = [f1_score(gt_label, to_labels(scores, t)) for t in cand_thresholds]\n",
    "ix = np.argmax(f1_scores_for_thrs)\n",
    "Th = cand_thresholds[ix]\n",
    "\n",
    "print('Threshold=%.2f, F-Score=%.5f' % Th, f1_scores_for_thrs[ix])\n",
    "print(\"precision: %.5f\" % precision_score(gt_label, to_labels(scores, Th)))\n",
    "print(\"recall: %.5f\" % recall_score(gt_label, to_labels(scores, Th)))\n",
    "print(\"f1_score: %.5f\" % f1_score(gt_label, to_labels(scores, Th)))\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "y_true = []\n",
    "y_score = []\n",
    "\n",
    "for res in Results:\n",
    "  tmp_arr = [0] * len(all_labels_finegrained)\n",
    "  tmp_arr[res[1]] = 1\n",
    "  y_true.append(tmp_arr)\n",
    "\n",
    "  tmp_arr = [0] * len(all_labels_finegrained)\n",
    "  tmp_arr[np.argmax(res[0])] = 1\n",
    "  y_score.append(tmp_arr)\n",
    "\n",
    "print('average_precision=%.5f, ACC=%.5f' % (average_precision_score(y_true, y_score), accuracy_score(y_true, y_score)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map fine grained-label scores to broad grained label scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_fine2broad = {} # can also be a list\n",
    "for ind,label in enumerate(all_labels_finegrained):  \n",
    "  mapping_fine2broad[ind] = all_labels_broad.index(' '.join(label.split('_')[1:]))\n",
    "\n",
    "mapping_broad2fine = {}\n",
    "for label,value in mapping_fine2broad.items(): \n",
    "  try:\n",
    "    mapping_broad2fine[value].append(label)\n",
    "  except KeyError:\n",
    "    mapping_broad2fine[value] = [label] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num_fine = len(all_labels_finegrained)\n",
    "class_num_broad = len(all_labels_broad)\n",
    "scores = []\n",
    "gt_label = []\n",
    "for res in Results:\n",
    "  tmp_arr = [0] * class_num_broad\n",
    "  tmp_arr[mapping_fine2broad[res[1]]] = 1\n",
    "  scores_tmp = []\n",
    "  for cnbi in range(class_num_broad):\n",
    "    all_scores_corr_broadLabel_cnbi = [res[0][corr_ind] for corr_ind in mapping_broad2fine[cnbi]]\n",
    "    scores_tmp.append(np.amax(all_scores_corr_broadLabel_cnbi))\n",
    "  scores += scores_tmp\n",
    "  gt_label += tmp_arr  \n",
    "\n",
    "assert len(scores) == len(gt_label)\n",
    "assert len(scores) == get_total_num_obj(val_dataset)*len(all_labels_broad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score,average_precision_score,accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Prediction Results:\")\n",
    "\n",
    "def to_labels(pos_probs, threshold):\n",
    " return [1 if nm > threshold else 0 for nm in pos_probs]\n",
    "\n",
    "cand_thresholds = [x / 100.0 for x in range(10, 95, 5)]\n",
    "f1_scores_for_thrs = [f1_score(gt_label, to_labels(scores, t)) for t in cand_thresholds]\n",
    "ix = np.argmax(f1_scores_for_thrs)\n",
    "Th = cand_thresholds[ix]\n",
    "\n",
    "print('Threshold=%.2f, F-Score=%.5f' % Th, f1_scores_for_thrs[ix])\n",
    "print(\"precision: %.5f\" % precision_score(gt_label, to_labels(scores, Th)))\n",
    "print(\"recall: %.5f\" % recall_score(gt_label, to_labels(scores, Th)))\n",
    "print(\"f1_score: %.5f\" % f1_score(gt_label, to_labels(scores, Th)))\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "y_true = []\n",
    "y_score = []\n",
    "\n",
    "for res in Results:\n",
    "  tmp_arr = [0] * len(all_labels_broad)\n",
    "  tmp_arr[res[1]] = 1\n",
    "  y_true.append(tmp_arr)\n",
    "\n",
    "  tmp_arr = [0] * len(all_labels_broad)\n",
    "  tmp_arr[np.argmax(res[0])] = 1\n",
    "  y_score.append(tmp_arr)\n",
    "\n",
    "print('average_precision=%.5f, ACC=%.5f' % (average_precision_score(y_true, y_score), accuracy_score(y_true, y_score)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broad Labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: currently averaging over all train data to get target_feature embeddings => try few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge for broad\n",
    "dict_broad = {}\n",
    "for key,value in dict_all.items():\n",
    "  broad_label = ' '.join(key.split('_')[1:])\n",
    "  try:\n",
    "    dict_broad[broad_label].append(value)\n",
    "  except KeyError:\n",
    "    dict_broad[broad_label] = [value]\n",
    "\n",
    "dict_broad_mean_feature_vecs = {}\n",
    "for key in dict_broad.keys():\n",
    "  dict_broad_mean_feature_vecs[key] = np.concatenate(dict_broad[key],axis=0).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = []\n",
    "for label in all_labels_broad:\n",
    "  target_features.append(dict_broad_mean_feature_vecs[label])\n",
    "target_features = torch.from_numpy(np.stack(target_features, axis=0)).type(torch.float16).to(device)\n",
    "\n",
    "Results_path = \"/content/drive/MyDrive/ApplicationProject/Results\"\n",
    "output_name = \"clip_gt_fewShot_broad_allTrain_Result.pkl\"\n",
    "if os.path.isfile(os.path.join(Results_path,output_name)):\n",
    "  from datetime import datetime\n",
    "  tmp_str = str(datetime.now()).split(' ')\n",
    "  output_name = output_name.split('.')[0] + '_' + tmp_str[0] + '_' + tmp_str[1].split('.')[0].replace(\":\", \"-\") +\".pkl\"\n",
    "output_dir = os.path.join(Results_path,output_name)\n",
    "\n",
    "Results = Calculate_Scores(clip_model,preprocess,val_dataset,target_features,all_labels_finegrained,output_dir,device,False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation - Broad labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "gt_label = []\n",
    "for res in Results:\n",
    "  tmp_arr = [0] * len(all_labels_broad)\n",
    "  tmp_arr[res[1]] = 1\n",
    "  scores += res[0]\n",
    "  gt_label += tmp_arr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images: \",len(val_dataset.get_num_imgs())) \n",
    "print(\"Number of objects: \",get_total_num_obj(val_dataset)) \n",
    "print(\"Number of classes: \",len(all_labels_finegrained))\n",
    "assert len(scores) == len(gt_label)\n",
    "assert len(scores) == get_total_num_obj(val_dataset)*len(all_labels_broad)\n",
    "print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score,average_precision_score,accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Prediction Results:\")\n",
    "\n",
    "def to_labels(pos_probs, threshold):\n",
    " return [1 if nm > threshold else 0 for nm in pos_probs]\n",
    "\n",
    "cand_thresholds = [x / 100.0 for x in range(10, 95, 5)]\n",
    "f1_scores_for_thrs = [f1_score(gt_label, to_labels(scores, t)) for t in cand_thresholds]\n",
    "ix = np.argmax(f1_scores_for_thrs)\n",
    "Th = cand_thresholds[ix]\n",
    "\n",
    "print('Threshold=%.2f, F-Score=%.5f' % Th, f1_scores_for_thrs[ix])\n",
    "print(\"precision: %.5f\" % precision_score(gt_label, to_labels(scores, Th)))\n",
    "print(\"recall: %.5f\" % recall_score(gt_label, to_labels(scores, Th)))\n",
    "print(\"f1_score: %.5f\" % f1_score(gt_label, to_labels(scores, Th)))\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "y_true = []\n",
    "y_score = []\n",
    "\n",
    "for res in Results:\n",
    "  tmp_arr = [0] * len(all_labels_broad)\n",
    "  tmp_arr[res[1]] = 1\n",
    "  y_true.append(tmp_arr)\n",
    "\n",
    "  tmp_arr = [0] * len(all_labels_broad)\n",
    "  tmp_arr[np.argmax(res[0])] = 1\n",
    "  y_score.append(tmp_arr)\n",
    "\n",
    "print('average_precision=%.5f, ACC=%.5f' % (average_precision_score(y_true, y_score), accuracy_score(y_true, y_score)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
