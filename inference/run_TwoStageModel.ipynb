{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/uonat/SS2023_DI-Lab_Precitaste.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd SS2023_DI-Lab_Precitaste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install . &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Proposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distutils.core\n",
    "import sys,os\n",
    "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'  &> /dev/null\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])} &> /dev/null\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install timm &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.VitDet import load_model as load_VitDet,Draw_pred_BB,Draw_original_seg\n",
    "model_path =\"/content/drive/MyDrive/ApplicationProject/Models/model2.pkl\"\n",
    "config_path = \"/content/SS2023_DI-Lab_Precitaste/detectron2/projects/ViTDet/configs/LVIS/mask_rcnn_vitdet_h_100ep.py\"\n",
    "vitDet_model = load_VitDet(model_path,config_path) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An image sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(\"sample.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "img = transforms.PILToTensor()(Image.open(\"CLIP.png\").convert(\"RGB\"))\n",
    "#Image.open(\"CLIP.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Obj proposels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [{'image':img.to(device)}]\n",
    "\n",
    "vitDet_model.to(device)\n",
    "vitDet_model.eval()\n",
    "with torch.no_grad():\n",
    "  model_result=vitDet_model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#Draw_pred_BB(np.asarray(img.movedim(0,-1))[:, :, ::-1],model_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove overlapping Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.non_maximum_suppression import run_nms \n",
    "\n",
    "picked_boxes, picked_score = run_nms(\n",
    "    model_result[0]['instances'].pred_boxes.tensor.to(torch.int).tolist(),\n",
    "    model_result[0]['instances'].scores.tolist(),\n",
    "    img,\n",
    "    0.2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop Obj proposels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "def Draw_pred_BB_editted(img,p_b,p_s):\n",
    "  final_img = img.copy()\n",
    "  for row_index in range(len(p_b)):\n",
    "    if p_s[row_index] > 0.5:\n",
    "      x1,y1,x2,y2 = p_b[row_index]\n",
    "      cv2.rectangle(final_img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "  cv2_imshow(final_img)\n",
    "\n",
    "Draw_pred_BB_editted(np.asarray(img.movedim(0,-1))[:, :, ::-1],picked_boxes,picked_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = []\n",
    "for i in  picked_boxes:\n",
    "  x1,y1,x2,y2 = i\n",
    "  #cv2_imshow( np.asarray(img.movedim(0,-1))[y1:y2,x1:x2, ::-1])\n",
    "  cropped_images.append(np.asarray(img.movedim(0,-1))[y1:y2,x1:x2, ::-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply CLIP on  cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy regex tqdm &> /dev/null\n",
    "!pip install git+https://github.com/openai/CLIP.git &> /dev/null\n",
    "\n",
    "from models.CLIP import available_clip_models,load_model as load_clip,tokenize_text\n",
    "#available_clip_models()\n",
    "clip_model, preprocess = load_clip(\"ViT-B/32\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"a photo of {}\".format(\"a chocolate bar\")\n",
    "text_p = tokenize_text(input_text,device)\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.encode_text(text_p)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#TODO all images at once in a batch\n",
    "similarities = []\n",
    "for cropped_image in cropped_images:\n",
    "  img_p = preprocess(Image.fromarray(np.uint8(cropped_image.copy()))).unsqueeze(0).to(device)\n",
    "  with torch.no_grad():\n",
    "    image_features = clip_model.encode_image(img_p)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "  similarities.append(image_features @ text_features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresh hold if more than this, object detected "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "for i in cropped_images:\n",
    "  image = preprocess(Image.fromarray(np.uint8(i.copy()))).unsqueeze(0).to(device)\n",
    "  Label_names = [\"an orange soda bottle\", \"a chocolate bar\", \"a bottle cap\",\"a lemon\"]\n",
    "  text = clip.tokenize(Label_names).to(device)\n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "      image_features = model_clip.encode_image(image)\n",
    "      text_features = model_clip.encode_text(text)\n",
    "      \n",
    "      logits_per_image, logits_per_text = model_clip(image, text)\n",
    "      probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "      ## OR ##\n",
    "      # Pick the top 5 most similar labels for the image\n",
    "      #image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "      #text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "      #similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "      #values, indices = similarity[0].topk(5)\n",
    "  # Print the result\n",
    "  #print(\"\\nTop predictions:\\n\")\n",
    "  #for value, index in zip(values, indices):\n",
    "    #print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "\n",
    "\n",
    "  print (\"Label names:\",Label_names)\n",
    "  print(\"Label probs:\", probs[0])\n",
    "  cv2_imshow(i)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
